{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "516fa71e",
   "metadata": {},
   "source": [
    "# YouTube Trailer Sentiment for WBD Titles\n",
    "\n",
    "Pull trailer comments via YouTube Data API and run DistilBERT sentiment.\n",
    "\n",
    "**Contact:** Pablo Monteros — [GitHub](https://github.com/Pmonteros8) • [LinkedIn](https://www.linkedin.com/in/pmonteros/) • [Email](mailto:Pablo.monterosj@gmail.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409f1239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from transformers import pipeline\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"config/.env\")\n",
    "YOUTUBE_KEY = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "assert YOUTUBE_KEY, \"Missing YOUTUBE_API_KEY in config/.env\"\n",
    "clf = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "def fetch_comments(video_id, cap=300):\n",
    "    yt = build(\"youtube\", \"v3\", developerKey=YOUTUBE_KEY)\n",
    "    res = yt.commentThreads().list(part=\"snippet\", videoId=video_id, maxResults=100, textFormat=\"plainText\").execute()\n",
    "    out = []\n",
    "    while True:\n",
    "        for item in res.get(\"items\", []):\n",
    "            top = item[\"snippet\"][\"topLevelComment\"][\"snippet\"]\n",
    "            out.append({\"video_id\": video_id, \"author\": top.get(\"authorDisplayName\"), \"text\": top.get(\"textDisplay\")})\n",
    "            if len(out)>=cap: break\n",
    "        if \"nextPageToken\" not in res or len(out)>=cap: break\n",
    "        res = yt.commentThreads().list(part=\"snippet\", videoId=video_id, maxResults=100, pageToken=res[\"nextPageToken\"], textFormat=\"plainText\").execute()\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "# Replace with real trailer IDs; fallback to sample file for offline testing\n",
    "video_ids = [\"dQw4w9WgXcQ\"]\n",
    "dfs = []\n",
    "for vid in video_ids:\n",
    "    try:\n",
    "        dfs.append(fetch_comments(vid, cap=300))\n",
    "    except Exception:\n",
    "        pass\n",
    "data = pd.concat(dfs, ignore_index=True) if dfs else pd.read_csv(\"data/youtube_comments_sample.csv\")\n",
    "preds = clf(data[\"text\"].astype(str).tolist(), truncation=True)\n",
    "data[\"sentiment\"] = [p[\"label\"] for p in preds]\n",
    "data[\"score\"] = [p[\"score\"] for p in preds]\n",
    "data.groupby(\"sentiment\").size()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
